<!DOCTYPE html>
<html>
<head>

<meta http-equiv="content-type" content="text/html;charset=UTF-8" />
<title>mousetrack</title>

<script type="text/x-mathjax-config">
  contents = '';

  MathJax.Hub.Config({
    TeX: {
      Macros: {
        mymatrix: ['{\\begin{bmatrix} #1 \\end{bmatrix}}',1],
        del: ['\\partial',0],
        by: ['\\over',0]
      },
      equationNumbers: {
        autoNumber: "all"
      }
    },
  });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">

<style type="text/css">

body {
  background-color: #ffffff;
  font-family: Georgia, serif;
  width: 50em;
  margin-left: auto;
  margin-right: auto;
}

h1 {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 300%;
}

.subtitle {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 150%;
}

.author {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #808080;
  text-align: center;
  font-size: 150%;
}

h2 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -70px;
  position: relative;   /* Needed so h2 .marker position works */
}

h2 .marker {
  background-color: #6aa84f;
  position: absolute;
  top: 0.2em;
  left: -140px;
  width: 50px;
  height: 0.8em;
  border-style: none;
}

h3 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -35px;
}

h4 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
}

table {
  border-collapse: collapse;
  padding: 0px;
}

table td {
  border: 1px solid #c0c0c0;
  vertical-align: top;
}

table td p {
  margin: 5px 5px;
}

pre {
  color: #0000ff;
  /* background-color: #e0ffff; */
  padding: 2px;
  font-size: 110%;
}

li p {
  margin: 0px 0px;
}

tt {
  color: #0000ff;
  font-size: 110%;
}

.contents_line_H2 {
  margin-left: 30px;
}

.contents_line_H3 {
  margin-left: 60px;
}

.contents_line_H4 {
  margin-left: 90px;
}

</style>

</head>
<body>

<!-- Main body of document inserted here -->
<script type='text/javascript'>__the_title__='Mousetrack'</script><h1 id='title'></h1>
<h1 class='author'>teenylasers</h1>
<div class='contents' id='contents'></div>
<h2 class='section_entry' id='1.'><span class='marker'></span>1. General Bayesian Estimation</h2>
<p>Define \(\beta_k(s_k)\) is the belief function, where we believe the state \(s\) is likely to be at time \(k\). \(P(m, s)\) is the joint probability density of getting a measurement \(m\) given the state \(s\). \(P(s_k, s_{k-1})\) is the joint probability density of finding state \(s_k\) at time \(k\) given \(s_{k-1}\).</p>
\[\begin{align}
\beta_k(s_k) &amp;= P_k(m_k, s_k)\int{\beta_{k-1}(s_{k-1})P_k(s_k, s_{i-1}) ds_{k-1}} \nonumber \\
   &amp;= P_k(m_k, s_k) \beta_{k|k-1}(s_k) \label{eq:general_bayes}
\end{align}\]
<p>In recursive implementation, \(\beta_{k|k-1}\) predicts the believe function based on the previous belief function \(\beta_{k-1}(s_{k-1})\) and the state evolution model \(P{s_k, s_{k-1}}\). It is then adjusted using the measurement model \(P(m_k, s_k)\) to arrive at the latest belief function \(\beta_k(s_k)\).</p>
<h2 class='section_entry' id='2.'><span class='marker'></span>2. Kalman Filter</h2>
<h3 class='section_entry' id='2.1.'>2.1. Derivation from general Bayesian estimation</h3>
<p>The belief function \(\beta\) describes the probability distribution of where \(s_k\) is in the state coordinates. For computational convenience, we model \(\beta\), \(P_k(m_k, s_k)\), and \(P_k(s_k, s_{k-1})\) each as a multivariate Gaussian. </p>
\[\begin{align}
P_k(s_k, s_{k-1}) &amp;= \mathcal{N}(As_{k-1}, Q, s_k) \nonumber \\
P_k(m_k, s_k) &amp;= \mathcal{N}(Cs_k, R, m_k) \nonumber \\
\beta(s_k) &amp;= \mathcal{N}(\mu_k, \Sigma_k, s_k)
\end{align}\]
<h3 class='section_entry' id='2.2.'>2.2. The model</h3>
<p>Apply Kalman Filter for a single track. For long-range autonomous driving tracking problems, we can assume a 2D tracking space, where there is no need for z or elevation information.</p>
<p>The state is modeled by </p>
\[\begin{align}s_{k+1} = As_k + w_k + d_{k+1|k}\end{align}\]
<p>where \(w_k\) represents the process noise, estimated by a Gaussian \(\mathcal{N}(0, Q)\), and \(d_{k+1|k}\) denotes a deterministic input such as relative position change due to own-ship motion.</p>
<p>Measurements are modeled by </p>
\[\begin{align}m_k = Cs_k + v_k\end{align}\]
<p>where \(v_k\) is the measurement noise, estimated by a Gaussian \(\mathcal{N}(0, R)\).</p>
<p>We want to estimate the belief function of \(s\), \(\beta(s_k) = \mathcal{N}(\mu_k, \Sigma_k, s_k)\)</p>
<p>From the previous belief function \(\beta(s_{k-1})\), we expect the unfiltered estimation of the current state is described by </p>
\[\begin{align}\hat{\mu}_{k|k-1} = A\mu_{k-1}\end{align}\]
\[\begin{align}\hat{\Sigma}_{k|k-1} = A\Sigma_{k-1}A^T + Q\end{align}\]
<p>The Kalman gain is </p>
\[\begin{align}\label{eq:kf_gain} K_k = \hat{\Sigma}_{k|k-1}C^T[C\hat{\Sigma}_{k|k-1}C^T + R]^{-1}\end{align}\]
<p>The filtered state is </p>
\[\begin{align}\mu_{k|k} = \hat{\mu}_{k|k-1} + K_k\tilde{m}_k\end{align}\]
\[\begin{align}\label{eq:kf_cov} \Sigma(k|k) = [\mathcal{I} - K_kC] \hat{\Sigma}_{k|k-1}\end{align}\]
<p>where \(\tilde{m} = m(k) - C\hat{\mu}_{k|k-1}\) is the unfiltered measurement residual, i.e. the innovation, and its covariance is \(S_k = C\hat{\Sigma}_{k|k-1}C^T + R\).</p>
<p>Finally, future state predictions are given by </p>
\[\begin{align}\mu_{k+1|k} = A\mu_{k|k}\end{align}\]
\[\begin{align}\Sigma_{k+1|k} = A\Sigma_{k|k}A^T + Q\end{align}\]
<h3 class='section_entry' id='2.3.'>2.3. Implementations</h3>
<h3 class='section_entry' id='kalman_model1'>2.4.  Model 1. Nonlinear measurement matrix \(C\), Taylor expansion for measurement noise covariance \(R\)</h3>
<p>Radar measurements are in polar coordinates. We choose to track in cartesian coordinates for a more intuitive model of target motion. For a radar scan interval of time \(T\), target motions are simply </p>
\[\begin{align}
x_{k+1} = x_k + T\dot{x}_k + \frac{T^2}{2}\ddot{x}_k \nonumber \\
y_{k+1} = y_k + T\dot{y}_k + \frac{T^2}{2}\ddot{y}_k \label{eq:kinetics}
\end{align}\]
<p>The state update model is </p>
\[\begin{align} \label{eq:const_v_process}
s = \begin{bmatrix} x \\ \dot{x} \\ y \\ \dot{y} \end{bmatrix},\quad
A = \begin{bmatrix} 1 &amp; T &amp; 0 &amp; 0 \\
                    0 &amp; 1 &amp; 0 &amp; 0 \\
		    0 &amp; 0 &amp; 1 &amp; T \\
		    0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}
\end{align}\]
<p>The measurement model that bridges polar to cartesian coordinates is </p>
\[\begin{align}
m_k &amp;= Cs_k \nonumber \\
\begin{bmatrix} x \\ y \\ r\dot{r} \end{bmatrix} = \begin{bmatrix} -r \sin(\theta) \\ r \cos(\theta) \\ r\dot{r} \end{bmatrix} &amp;=
\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\
		0 &amp; 0 &amp; 1 &amp; 0 \\
		\dot{x} &amp; x &amp; \dot{y} &amp; y
\end{bmatrix}
\begin{bmatrix} x \\ \dot{x} \\ y \\ \dot{y} \end{bmatrix}
\end{align}\]
<p><b> TODO</b>: the form of \(m = Cs\) comes from [Radar Data Processing by Farina and Studer, Section 4.6], derive, and alternative treatments, e.g. EKF or UKF?</p>
<p>Notes: </p>
<ul>
<li><p>Acceleration is not included in the state model for now. </p>

</ul>
<h4 class='section_entry' id='2.4.1.'>2.4.1. Process covariance</h4>
<p>Process noise may include white noise from e.g. the RF mixer, correlated noise e.g. angular scintillation, and as a way to treat target acceleration or maneuvering using the Singer model. For now, we use the Singer model for slowly maneuvering targets, that is, sampling time \(T \ll \tau_m\) the maneuver time constant. The resulting covariance is stated here without derivation, adapted from [Blackman book, Page 33].</p>
<p><b> TODO</b>: derive, model other noise</p>
\[\begin{align}
Q = \frac{2\sigma^2_m}{\tau_m}
\begin{bmatrix}
\frac{T^5}{20} &amp; \frac{T^4}{8} &amp; 0 &amp; 0 \\
\frac{T^4}{8} &amp; \frac{T^3}{3} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \frac{T^5}{20} &amp; \frac{T^4}{8} \\
0 &amp; 0 &amp; \frac{T^4}{8} &amp; \frac{T^3}{3}
\end{bmatrix}
\end{align}\]
<h4 class='section_entry' id='2.4.2.'>2.4.2. Coordinate transform for measurement covariance</h4>
<p>Radar measurements typically consists of \(\{r, \dot{r}, \theta\}\) with variances \(\{\sigma^2_r, \sigma^2_{\dot{r}}, \sigma^2_\theta\}\). We wish to calculate the covariance of the measurement noise \(\mathcal{N}(0, R)\), where </p>
\[\begin{align}R =
\begin{bmatrix}
\sigma^2_{xx} &amp; \sigma^2_{xy} &amp; \sigma^2_{xr\dot{r}} \\
\sigma^2_{xy} &amp;  \sigma^2_{yy} &amp; \sigma^2_{yr\dot{r}} \\
\sigma^2_{xr\dot{r}} &amp; \sigma^2_{yr\dot{r}} &amp; \sigma^2_{r\dot{r}r\dot{r}}
\end{bmatrix}\end{align}\]
<p>is a function of the raw radar measurements' variances.</p>
<p>Consider a general transformation of a random variable \(\mathbf{x}\), with mean \(\bar{\mathbf{x}}\) and covariance \(\mathbf{\Sigma_x}\), to a new random variable \(\mathbf{y}\) by the nonlinear function \(f\) </p>
\[\begin{align}\mathbf{y} = f(\mathbf{x})\end{align}\]
<p>We apply Taylor expansion around \(\bar{\mathbf{x}}\), using subscript-summation notation, </p>
\[\begin{align}
f_i(\mathbf{x}) &amp;= f_i(\bar{\mathbf{x}}) + \frac{\partial f_i}{\partial x_j}(x_j-\bar{x}_j) + \frac{1}{2}\frac{\partial^2f_i}{\partial x_j \partial x_k}(x_j - \bar{x}_j)(x_k - \bar{x}_k) \nonumber \\
&amp;= f_i(\bar{\mathbf{x}}) + \mathbf{J}_i(\mathbf{x} - \bar{\mathbf{x}}) + \frac{1}{2}(\mathbf{x} - \bar{\mathbf{x}})^T\mathbf{H}_i(\mathbf{x} - \bar{\mathbf{x}})
\end{align}\]
<p>where \(\mathbf{J}_i\) is the i-th row of the Jacobian matrix, and \(\mathbf{H}_i\) is the i-th plane of the 3-dimensional Hessian tensor. Assume that \((\mathbf{x} - \bar{\mathbf{x}})\) is zero-mean with covariance \(\mathbf{\Sigma_x}\), take the expectation value to get \(\bar{\mathbf{y}}\) and \(\mathbf{\Sigma_y}\) </p>
\[\begin{align}
\bar{\mathbf{y}} = E(\mathbf{y}) &amp;= E(f(\bar{\mathbf{x}})) + \mathbf{J} E(\mathbf{x} - \bar{\mathbf{x}}) + \frac{1}{2}E((\mathbf{x} - \bar{\mathbf{x}})^T\mathbf{H}(\mathbf{x} - \bar{\mathbf{x}})) \nonumber \\
&amp;= f(\bar{\mathbf{x}}) + \frac{1}{2}E((\mathbf{x} - \bar{\mathbf{x}})^T\mathbf{H}(\mathbf{x} - \bar{\mathbf{x}})) \nonumber \\
&amp;\approx f(\bar{\mathbf{x}})
\end{align}\]
\[\begin{align}
\mathbf{\Sigma_y} = E((\mathbf{y} - \bar{\mathbf{y}})(\mathbf{y} - \bar{\mathbf{y}})^T) &amp;= \mathbf{J}E((\mathbf{x} - \bar{\mathbf{x}})(\mathbf{x} - \bar{\mathbf{x}})^T)\mathbf{J}^T \nonumber \\
&amp;= \mathbf{J}\mathbf{\Sigma_x}\mathbf{J}^T \label{eq:y_var}
\end{align}\]
<p>Apply \eqref{eq:y_var} to the measurement model, we have \(\mathbf{x} = \{r, \theta, \dot{r}\}\), \(f(\mathbf{x}) = \{-r\sin\theta, r\cos\theta, r\dot{r}\}\), assume the variances of \(\{r, \dot{r}, \theta\}\) are not correlated, </p>
\[\begin{align}
R &amp;= \mathbf{J}
\begin{bmatrix}
\sigma^2_r &amp; 0 &amp; 0 \\
0 &amp; \sigma^2_\theta &amp; 0 \\
0 &amp; 0 &amp; \sigma^2_\dot{r}
\end{bmatrix} \mathbf{J}^T \nonumber \\
&amp;= \begin{bmatrix}
\sigma^2_r\sin^2\theta + r^2\sigma^2_\theta\cos^2\theta &amp; \cos\theta\sin\theta(\sigma^2_r - r^2\sigma^2_\theta) &amp; -\dot{r}\sigma^2_r\sin\theta \\
\cos\theta\sin\theta(\sigma^2_r - r^2\sigma^2_\theta) &amp; r^2\sigma^2_\theta\sin^2\theta + \sigma^2_r\cos^2\theta &amp; \dot{r}\sigma^2_r\cos\theta \\
-\dot{r}\sigma^2_r\sin\theta &amp; \dot{r}\sigma^2_r\cos\theta &amp; \dot{r}^2\sigma^2_r + r^2\sigma^2_\dot{r}
\end{bmatrix}
\end{align}\]
<h3 class='section_entry' id='kalman_model2'>2.5.  Model 2. Nonlinear measurement model \(m = c(s)\), extended Kalman filter for belief covariance update \(C\hat{\Sigma}C^T\)</h3>
<p>The process model is unchanged from <a href='#model1'>Model 1</a> above, described by \eqref{eq:kinetics} and \eqref{eq:const_v_process}. The measurement vector is the radar's direct measurement, </p>
\[\begin{align}m_k &amp;= c(s_k) \nonumber \\
\begin{bmatrix}r \\ \theta \\ \dot{r}\end{bmatrix} &amp;=
\begin{bmatrix}
\sqrt{x^2 + y^2} \\
\arctan{(-x/y)} \\
-\dot{x}\sin{\theta} + \dot{y}\cos{\theta}
\end{bmatrix} \label{eq:c_fxn}\end{align}\]
<p>The measurement noise covariance is </p>
\[\begin{align}
R = \begin{bmatrix}
\sigma^2_r &amp; 0 &amp; 0 \\
0 &amp; \sigma^2_\theta &amp; 0
\\ 0 &amp; 0 &amp; \sigma^2_\dot{r}
\end{bmatrix}\end{align}\]
<p>Because we no longer have a matrix \(C\) to transform the state vector \(s\) to the measurement \(m\), calculations of the Kalman gain \eqref{eq:kf_gain} and the belief function covariance \eqref{eq:kf_cov} is now given by </p>
\[\begin{align}K_k = \hat{\Sigma}_{k|k-1}\mathbf{J}_c^T[\mathbf{J}_c\hat{\Sigma}_{k|k-1}\mathbf{J}_c^T + R]^{-1}\end{align}\]
\[\begin{align}\Sigma(k|k) = [\mathcal{I} - K_k\mathbf{J}_c] \hat{\Sigma}_{k|k-1}\end{align}\]
<p>where </p>
\[\begin{align}
\mathbf{J}_c &amp;= \frac{\partial c(s)}{\partial s} \nonumber \\
 &amp;= \begin{bmatrix}
 \frac{x}{\sqrt{x^2+y^2}} &amp; 0 &amp; \frac{y}{\sqrt{x^2+y^2}} &amp; 0 \\
 -\frac{1}{(1+\frac{x^2}{y^2})y} &amp; 0 &amp; \frac{x}{(1+\frac{x^2}{y^2})y^2} &amp; 0 \\
 -\frac{x^2\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^3} + \frac{\dot{x}}{\sqrt{1+\frac{x^2}{y^2}}y} - \frac{x\dot{y}}{(1+\frac{x^3}{y^2})^{3/2}y^2} &amp;
 \frac{x}{\sqrt{1+\frac{x^2}{y^2}}y} &amp;
 \frac{x^3\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^4}-\frac{x^2\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^3}+\frac{x^2\dot{y}}{(1+\frac{x^3}{y^2})^{3/2}y^3} &amp;
 \frac{1}{\sqrt{1+\frac{x^2}{y^2}}} \label{eq:ekf_jc}
\end{bmatrix}
\end{align}\]
<h3 class='section_entry' id='kalman_model3'>2.6.  Model 3. Extended Kalman filter with constant acceleration kinematic model</h3>
<p>We augment <a href='#kalman_model2'>Model 2</a> above to include acceleration in the state, that is, </p>
\[\begin{align}
s = \begin{bmatrix} x \\ \dot{x} \\ \ddot{x} \\ y \\ \dot{y} \\ \ddot{y} \end{bmatrix},
A = \begin{bmatrix}
    1 &amp; T &amp; \frac{T^2}{2} &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; T &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; T &amp; \frac{T^2}{2} \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; T \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
    \end{bmatrix}
\end{align}\]
<p>The measurement model is still described by \eqref{eq:c_fxn}; however, the Jacobian linearization now becomes </p>
\[\begin{align}
\mathbf{J}_c = \begin{bmatrix}
 \frac{x}{\sqrt{x^2+y^2}} &amp; 0 &amp; 0 &amp; \frac{y}{\sqrt{x^2+y^2}} &amp; 0 &amp; 0 \\
 -\frac{1}{(1+\frac{x^2}{y^2})y} &amp; 0 &amp; 0 &amp; \frac{x}{(1+\frac{x^2}{y^2})y^2} &amp; 0 &amp; 0 \\
 -\frac{x^2\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^3} + \frac{\dot{x}}{\sqrt{1+\frac{x^2}{y^2}}y} - \frac{x\dot{y}}{(1+\frac{x^3}{y^2})^{3/2}y^2} &amp;
 \frac{x}{\sqrt{1+\frac{x^2}{y^2}}y} &amp; 0 &amp;
 \frac{x^3\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^4}-\frac{x^2\dot{x}}{(1+\frac{x^3}{y^2})^{3/2}y^3}+\frac{x^2\dot{y}}{(1+\frac{x^3}{y^2})^{3/2}y^3} &amp;
 \frac{1}{\sqrt{1+\frac{x^2}{y^2}}} &amp; 0
\end{bmatrix}
\end{align}\]
<h3 class='section_entry' id='2.7.'>2.7. Implementation results and to-do's</h3>
<ol>
<li><p>With <a href='#kalman_model1'>Model 1</a>, velocity estimation is consistently poor or slow to catch up with ground truth. Either tune covariance matrices, or implement extended Kalman filter to see whether the measurement model is the root cause. </p>
<li><p>How to evaluate and compare models or filters? </p>
<li><p>With <a href='#kalman_model3'>Model 3</a>, the filter is unstable, seemingly because acceleration estimates swing wildly for a few time periods before it converges, by which time position estimation may have become irrecoverably wrong. Is the solution a better tuned covariance \(Q\) or to relate acceleration with measurement, i.e. modify \eqref{eq:c_fxn}. </p>
<li><p>Visualize covariance, start with point cloud of \(\{r, \theta, \dot{r}\}\) using \(\{\sigma_r, \sigma_\theta, \sigma_\dot{r}\}\), coordinate transform to \(\{x, y, r\dot{r}\}\) or \(\{x, \dot{x}, \ddot{x}, y, \dot{y}, \ddot{y}\}\). </p>

</ol>
<h2 class='section_entry' id='3.'><span class='marker'></span>3. Probabilistic Hypothesis Density (PHD) Filter</h2>
<h3 class='section_entry' id='3.1.'>3.1. Derivation from general Bayesian estimation</h3>
<p>Following from the general Bayesian estimator \eqref{eq:general_bayes},</p>
<p><b> Prediction</b></p>
\[\begin{align} \label{eq:phd_predict}
\beta_{k|k-1}(s_k) = b(s_k) + \int P(s_k, s_{k-1})P_s\beta_{k-1}(s_{k-1}) ds_{k-1}
\end{align}\]
<p>where \(P(s_k, s_{k-1})\) is the probability density of a transition from \(s_{k-1}\) to \(s_k\), \(P_s\) is the target survival probability at time \(k\) given its previous state \(s_{k-1}\), \(b(s_k)\) is the target birth probability. In contrast to the Kalman filter representation, \(s\) encapsulates all targets in the observation space. Note, in many papers, the model contains an additional term for the probability of targets being spawn at time \(k\), we will not consider new targets spawning from existing targets to be different from general target births.</p>
<p><b> Update</b> </p>
\[\begin{align} \label{eq:phd_update}
\beta_{k|k}(s_k) &amp;= \beta_{k|k-1}(s_k) \left[ (1 - P_d) + \sum_{j=1}^M \frac{P_d P(m_j, s_k)}{\kappa(m_j)\mu_{FA} + \int P_d P(m_j, s_k)\beta_{k|k-1}(s_k) ds_k} \right]
\end{align}\]
<p>where \(P_d\) is the probability of detection, \(M\) is the number of measurements, \(j\) is the index that keeps track of all \(M\) measurements, \(P(m_j, s_k)\) is the <em>likelihood</em>, i.e. the joint probability density of getting the \(j\)-th measurement given state \(s_k\), \(\kappa(m_s)\) is the clutter model, the probability distribution of clutter points over the observation space, \(\mu_{FA}\) is the Poisson false alarm rate.</p>
<h3 class='section_entry' id='3.2.'>3.2. Implement using Gaussian mixture model</h3>
<p>So far, we have made no assumption about the form the various probability functions take. To enable a closed-form model, we use the Gaussian mixture model:</p>
\[\begin{align}P(s_k, s_{k-1}) &amp;= \mathcal{N}(As_{k-1}, Q, s_k) \nonumber \\
P(m_k, s_k) &amp;= \mathcal{N}(Cs_k, R, m_k) \nonumber \\
b(s_k) &amp;= \sum_{i=1}^{J_b} w_{b,i}\mathcal{N}(\mu_b, \Sigma_b, s_k)
\end{align}\]
<p>The form of \(b_k\) is a generic weighted gaussian mixture. We also assume that \(P_s\) and \(P_d\) are both constant over time.</p>
<p>Since all input functions to the estimation of belief functions \(\beta\) are gaussians or mixture of gaussians, \(\beta\) also takes on the form of a mixture of gaussians, </p>
\[\begin{align}
\beta_k(s_k) = \sum_{i=1}^{J_\beta}w_{\beta,i}\mathcal{N}(\mu_{k,i}, \Sigma_{k,i}, s_k)
\end{align}\]
<p>Use the following identities and rules for Gaussian multiplication and integration </p>
\[\begin{align}\label{eq:gaussian_swap_variables}
\mathcal{N}(As+b, Q, x) = |A|\mathcal{N}(A^{-1}(x-b),A^{-1}RA^{-T},s)
\end{align}\]
\[\begin{align}
\mathcal{N}(\mu_1, \Sigma_1, x)\mathcal{N}(\mu_2, \Sigma_2, x) = \mathcal{N}(\mu_1, \Sigma_1+\Sigma_2, \mu_2) \mathcal{N}((\Sigma_1^{-1}+\Sigma_2^{-1})^{-1}(\Sigma_1^{-1}\mu_1 + \Sigma_2^{-1}\mu_2), (\Sigma_1^{-1}+\Sigma_2^{-1})^{-1}, x)
\end{align}\]
\[\begin{align} \label{eq:mat_inverse_rule}
(A^{-1} + B^{-1})^{-1} = A(A+B)^{-1}B = (I-B(A+B)^{-1})B
\end{align}\]
\[\begin{align} \label{eq:mat_integral_rule}
\int \mathcal{N}(Ax, Q, y)\mathcal{N}(\mu, \Sigma, x) dx = \mathcal{N}(A\mu, Q + A\Sigma A^T, y)
\end{align}\]
<p>Then \eqref{eq:phd_predict} becomes </p>
\[\begin{align}
\beta_{k|k-1}(s_k)
&amp;= b(s_k) + P_s\int \mathcal{N}(As_{k-1}, Q, s_k) \sum_{i=1}^{J_\beta}w_{\beta,i}\mathcal{N}(\mu_{k-1,i}, \Sigma_{k-1,i}, s_{k-1}) ds_{k-1} \nonumber \\
&amp;= b(s_k) + P_s\sum_{i=1}^{J_\beta} w_{\beta,i} \int \mathcal{N}(As_{k-1}, Q, s_k)\mathcal{N}(\mu_{k-1,i}, \Sigma_{k-1,i}, s_{k-1}) ds_{k-1} \nonumber \\
&amp;= \sum_{i=1}^{J_b} w_{b,i}\mathcal{N}(\mu_b, \Sigma_b, s_k) + P_s \sum_{i=1}^{J_\beta} w_{\beta,i} \mathcal{N}(A\mu_{k-1,i}, Q + A\Sigma_{k-1,i}A^T, s_k)  \label{eq:phd_predict_gmm} \\
&amp;= \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(\mu_{k|k-1,i}, \Sigma_{k|k-1,i}, s_k) \nonumber
\end{align}\]
<p>and \eqref{eq:phd_update} becomes </p>
\[\begin{align}
\beta_{k|k}(s_k)
&amp;= (1-P_d)\beta_{k|k-1}(s_k) + P_d \sum_{j=1}^M
    \frac{\beta_{k|k-1}(s_k)\mathcal{N}(Cs_k, R, m_j)}
         {\kappa(m_j)\mu_{FA} + P_d\int \mathcal{N}(Cs_k, R, m_j)\beta_{k|k-1}(s_k) ds_k} \nonumber \\
&amp;= (1-P_d)\beta_{k|k-1}(s_k) + P_d \sum_{j=1}^M
    \frac{\beta_{k|k-1}(s_k)\mathcal{N}(Cs_k, R, m_j)}
    	{\kappa(m_j)\mu_{FA} + P_d \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j)} \label{eq:phd_update_gmm}
\end{align}\]
<p>We notice that \(\mathcal{N}(Cs_k, R, m_j)\) in the numerator expresses the probability density in measurement space instead of state space. One way to map to state space is to use \eqref{eq:gaussian_swap_variables}, \(\mathcal{N}(Cs_k, R, m_j) = |C|\mathcal{N}(C^{-1}m_j, C^{-1}TC^{-T}, s_k)\). However, \(C\) is often not a square matrix, measurement space usually has lower dimensions than state space. Therefore, we cannot meaningfully calculate and use \(C^{-1}\). There are several ways to proceed:</p>
<ol>
<li><p>Eigen-decomposition <b> TODO</b>: how does it help? </p>
<li><p>Use numerically large covariance in directions that ill-characterized by the measurements </p>
<li><p>\eqref{eq:phd_update_gmm} can be expressed as a weighted sum of Kalman filters. The form of Kalman filter side-steps the need to invert \(C\).</p>

</ol>
<p><b> TODO</b>: what does the inverse of a rectangular \(C\) really mean? What happens when measurement space has lower or higher dimensions than state space?</p>
<h4 class='section_entry' id='3.2.1.'>3.2.1. The Gaussian mixture PHD filter as a weighted sum of Kalman filters</h4>
<p>We can re-write \eqref{eq:phd_update_gmm} as a sum of Kalman filter updates. We expand \(\beta_{k|k-1}(s_k)\), and setting aside the target birth probability distribution \(b(s_k)\) for now, since it is usually handled as a separate target initiation step in Kalman filter implementations. </p>
\[\begin{align}
\beta_{k|k}(s_k)
&amp;= (1-P_d)\beta_{k|k-1}(s_k) + P_d \sum_{j=1}^M
   \frac{\sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(\mu_{k|k-1,i}, \Sigma_{k|k-1,i}, s_k) \mathcal{N}(Cs_k, R, m_j)}
	{\kappa(m_j)\mu_{FA} + P_d \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j) } \nonumber
\end{align}\]
<p>First, we treat the gaussian multiplication in the numerator, </p>
\[\begin{align}
\mathcal{N}(Cs, R, m)\mathcal{N}(\hat{\mu}, \hat{\Sigma}, s)
&amp;= |C|\mathcal{N}(C^{-1}m, C^{-1}RC^{-T}, s)\mathcal{N}(\hat{\mu}, \hat{\Sigma}, s) \nonumber \\
&amp;= |C|\mathcal{N}(C^{-1}m, C^{-1}RC^{-T} + \hat{\Sigma}, \hat{\mu})
   \mathcal{N}\left (\left[(C^{-1}RC^{-T})^{-1} + \hat{\Sigma}^{-1}\right]^{-1} \left[ (C^{-1}RC^{-T})^{-1}C^{-1}m + \hat{\Sigma}^{-1}\hat{\mu} \right], \left[(C^{-1}RC^{-T})^{-1} + \hat{\Sigma}^{-1}\right]^{-1} , s \right) \label{eq:phd_update_numerator}
\end{align}\]
<p>For the covariance of the second gaussian in \eqref{eq:phd_update_numerator}, apply \eqref{eq:mat_inverse_rule}, </p>
\[\begin{align}
\Sigma
&amp;= \left[(C^{-1}RC^{-T})^{-1} + \hat{\Sigma}^{-1}\right]^{-1} \nonumber \\
&amp;= \left[ I - \hat{\Sigma}(C^{-1}RC^{-T} + \hat{\Sigma})^{-1} \right] \hat{\Sigma} \nonumber \\
&amp;= \left[ I - \hat{\Sigma}(C^{-1}(R+C\hat{\Sigma}C^T)C^-{T})^{-1} \right] \nonumber \hat{\Sigma} \\
&amp;= \left[ I - \hat{\Sigma}C^T(R+C\hat{\Sigma}C^T)^{-1}C \right] \hat{\Sigma} \nonumber \\
&amp;= (I-KC)\hat{\Sigma}
\end{align}\]
<p>where \(K\) is the Kalman filter, given by </p>
\[\begin{align}K = \hat{\Sigma}C^T(R+C\hat{\Sigma}C^T)^{-1}\end{align}\]
<p>For the mean of the second gaussian in \eqref{eq:phd_update_numerator}, </p>
\[\begin{align}
\mu
&amp;= \Sigma\left[ (C^{-1}RC^{-T})C^{-1}m + \hat{\Sigma}^{-1}\hat{\mu} \right] \nonumber \\
&amp;= \Sigma(C^TRm - \hat{\Sigma}^{-1}\hat{\mu}) = \hat{\mu} + K(m - C\hat{\mu})
\end{align}\]
<p>Simplifying the first gaussian, \eqref{eq:phd_update_numerator} becomes </p>
\[\begin{align}
\mathcal{N}(Cs_k, R, m_j)\mathcal{N}(\mu_{k|k-1,i}, \Sigma_{k|k-1,i}, s)
&amp;= \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j)\mathcal{N}(\mu_{k|k-1,i} + K(m_j - C\mu_{k|k-1,i}), (I-KC)\Sigma_{k|k-1,i}, s)
\end{align}\]
<p>Putting it all together, </p>
\[\begin{align}
\beta_{k|k}(s_k)
&amp;=(1-P_d)\beta_{k|k-1}(s_k) + P_d \sum_{j=1}^M
   \frac{\sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j)\mathcal{N}(\mu_{k|k-1,i} + K(m_j - C\mu_{k|k-1,i}), (I-KC)\Sigma_{k|k-1,i}, s_k)}
   	{\kappa(m_j)\mu_{FA} + P_d \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j)}\end{align}\]
<h4 class='section_entry' id='3.2.2.'>3.2.2. Model 1. EKF-style measurement model, \(P_s = 1\), \(P_d = 1\), \(\mu_{FA} = 0\)</h4>
<p>To start, we assume the probability of target survival and the probability of detection are both 1. For further simplification to test the model, we set false alarm distribution to 0. We follow the models used in extended Kalman filters. The process model is given by \eqref{eq:const_v_process}; the measurement model is \eqref{eq:c_fxn}. The prediction and update equations become </p>
\[\begin{align} \label{eq:phd_gmm1_predict}
\beta_{k|k-1}(s_k) = b(s_k)  + \sum_{i=1}^{J_{\beta, k-1}} w_{\beta,k-1,i} \mathcal{N}(A\mu_{k-1,i}, Q + A\Sigma_{k-1,i}A^T, s_k)
\end{align}\]
\[\begin{align} \label{eq:phd_gmm1_update}
\beta_{k|k}(s_k)
= \frac{\sum_{j=1}^M \sum_{i=1}^{J_{\beta, k-1}} w_{\beta,k|k-1,i} \mathcal{N}(C\mu_{k|k-1,i}, R+C\Sigma_{k|k-1,i}C^T, m_j)\mathcal{N}(\mu_{k|k-1,i} + K(m_j - C\mu_{k|k-1,i}), (I-KC)\Sigma_{k|k-1,i}, s_k)}
   	{\sum_{j=1}^M \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(c({\mu_{k|k-1,i}}), \mathbf{J}_c \Sigma_{k|k-1,i} \mathbf{J}_c^T + R, m_j)}
\end{align}\]
<p>where \(\mathbf{J}_c\) is given by \eqref{eq:ekf_jc}.</p>
<p>We make no assumption on where new targets are more likely to pop up, so we apply a constant target birth probability to measurements that suggest the possibility of a new target, \(b(s_k) = const\).</p>
<p><b> TODO:</b> </p>
<ol>
<li><p>In \eqref{eq:phd_gmm1_update}, the denominator gaussian's covariance is the residual covariance. Why? </p>
<li>
</ol>
<h4 class='section_entry' id='3.2.3.'>3.2.3. Model 2. Reinstate clutter and false alarm, \(\kappa(m_j)\) and \(\mu_{FA}\)</h4>
\[\begin{align}
\beta_{k|k}(s_k) = \frac{\sum_{j=1}^M \beta_{k|k-1}(s_k)\mathcal{N}(c(s_k), R, m_j)}
    	{\sum_{j=1}^M\kappa(m_j)\mu_{FA} + \sum_{j=1}^M \sum_{i=1}^{J_{\beta,k|k-1}} w_{\beta,k|k-1,i} \mathcal{N}(c(\mu_{k|k-1,i}), \mathbf{J}_c \Sigma_{k|k-1,i} \mathbf{J}_c^T + R, m_j)}
\end{align}\]


<script type="text/javascript">
// Fill in contents.
var c = document.getElementById("contents");
var elements = document.getElementsByClassName('section_entry');
if (c && elements) {
  var html = '<h2>Contents</h2>';
  for (var i = 0; i < elements.length; i++) {
    html += '<div class="contents_line_' +
            elements[i].nodeName +
            '"><a href="#' +
            elements[i].id + '">' + elements[i].innerHTML +
            '</a></div>';
  }
  c.innerHTML = html;
}

// Set title.
document.getElementById("title").innerHTML = __the_title__;
</script>

</body>
</html>
